{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81134ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xlrd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import boto3\n",
    "import botocore\n",
    "import requests\n",
    "import os\n",
    "from io import StringIO\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a4abda",
   "metadata": {},
   "source": [
    "## CLEANING AND UPLOADING FILES TO NEW BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88fe1098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Downloading 2020 file...\n",
      "Step 2: Creating DataFrame for 2020 file...\n",
      "Step 3: Renaming columns for 2020 file...\n",
      "Step 4: Exporting cleaned data for 2020...\n",
      "Step 5: Uploading cleaned data to https://inep-cleaned.s3.amazonaws.com/...\n",
      "Cleaning and uploading for 2020 is complete!\n",
      "\n",
      "Step 1: Downloading 2021 file...\n",
      "Step 2: Creating DataFrame for 2021 file...\n",
      "Step 3: Renaming columns for 2021 file...\n",
      "Step 4: Exporting cleaned data for 2021...\n",
      "Step 5: Uploading cleaned data to https://inep-cleaned.s3.amazonaws.com/...\n",
      "Cleaning and uploading for 2021 is complete!\n",
      "\n",
      "Step 1: Downloading 2022 file...\n",
      "Step 2: Creating DataFrame for 2022 file...\n",
      "Step 3: Renaming columns for 2022 file...\n",
      "Step 4: Exporting cleaned data for 2022...\n",
      "Step 5: Uploading cleaned data to https://inep-cleaned.s3.amazonaws.com/...\n",
      "Cleaning and uploading for 2022 is complete!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the S3 bucket URLs\n",
    "source_bucket_url = 'https://inep.s3.amazonaws.com/'\n",
    "destination_bucket_url = 'https://inep-cleaned.s3.amazonaws.com/'\n",
    "\n",
    "# List of years for your files\n",
    "years = ['2020', '2021', '2022']\n",
    "\n",
    "# Dictionary to store DataFrames for each year\n",
    "dfs = {}\n",
    "\n",
    "for year in years:\n",
    "    # Step 1: Download the file\n",
    "    print(f\"Step 1: Downloading {year} file...\")\n",
    "    source_key = f'MICRODADOS_ENEM_{year}.csv'\n",
    "    destination_key = f'summary_{source_key}'\n",
    "    \n",
    "    source_object_url = source_bucket_url + source_key\n",
    "    response = requests.get(source_object_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        with open(source_key, 'wb') as local_file:\n",
    "            local_file.write(response.content)\n",
    "    else:\n",
    "        print(f\"Failed to download {year} file. Status code: {response.status_code}\")\n",
    "\n",
    "    # Step 2: Create a DataFrame and select specific columns\n",
    "    print(f\"Step 2: Creating DataFrame for {year} file...\")\n",
    "    columns_to_keep = [\n",
    "        'NU_INSCRICAO', 'NU_ANO', 'TP_FAIXA_ETARIA', 'TP_SEXO', 'TP_ESTADO_CIVIL', 'TP_COR_RACA',\n",
    "        'TP_NACIONALIDADE', 'TP_ST_CONCLUSAO', 'TP_ANO_CONCLUIU', 'TP_ESCOLA', 'TP_ENSINO',\n",
    "        'IN_TREINEIRO', 'CO_MUNICIPIO_PROVA', 'NO_MUNICIPIO_PROVA', 'CO_UF_PROVA', 'SG_UF_PROVA',\n",
    "        'NU_NOTA_CN', 'NU_NOTA_CH', 'NU_NOTA_LC', 'NU_NOTA_MT', 'NU_NOTA_REDACAO'\n",
    "    ]\n",
    "    \n",
    "    df = pd.read_csv(source_key, sep=';', encoding='latin1', usecols=columns_to_keep)\n",
    "    \n",
    "    # Step 3: Rename columns\n",
    "    print(f\"Step 3: Renaming columns for {year} file...\")\n",
    "    df.rename(columns={\n",
    "        'NU_INSCRICAO': 'id', 'NU_ANO': 'year', 'TP_FAIXA_ETARIA': 'age_code', 'TP_SEXO': 'sex_code',\n",
    "        'TP_ESTADO_CIVIL': 'civil_code', 'TP_COR_RACA': 'etinicity_code', 'TP_NACIONALIDADE': 'nationality_code',\n",
    "        'TP_ST_CONCLUSAO': 'conclusion_code', 'TP_ANO_CONCLUIU': 'concluion_year', 'TP_ESCOLA': 'school_code',\n",
    "        'TP_ENSINO': 'teaching_code', 'IN_TREINEIRO': 'is_training', 'CO_MUNICIPIO_PROVA': 'municipality_code',\n",
    "        'NO_MUNICIPIO_PROVA': 'municipality_name', 'CO_UF_PROVA': 'uf_code', 'SG_UF_PROVA': 'uf_name',\n",
    "        'NU_NOTA_CN': 'cn_score', 'NU_NOTA_CH': 'ch_score', 'NU_NOTA_LC': 'lc_score',\n",
    "        'NU_NOTA_MT': 'mt_score', 'NU_NOTA_REDACAO': 'essay_score'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    dfs[year] = df  # Store the DataFrame in the dictionary\n",
    "    \n",
    "    # Step 4: Export the cleaned data to a new CSV\n",
    "    print(f\"Step 4: Exporting cleaned data for {year}...\")\n",
    "    df.to_csv(destination_key, index=False)\n",
    "    \n",
    "    # Step 5: Upload the CSV to the destination bucket\n",
    "    print(f\"Step 5: Uploading cleaned data to {destination_bucket_url}...\")\n",
    "    destination_object_url = destination_bucket_url + destination_key\n",
    "    with open(destination_key, 'rb') as local_file:\n",
    "        response = requests.put(destination_object_url, data=local_file)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(f\"Cleaning and uploading for {year} is complete!\\n\")\n",
    "    else:\n",
    "        print(f\"Failed to upload {year} file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c288d43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Cleanup: Delete the downloaded files\n",
    "os.remove(source_key)\n",
    "os.remove(destination_key)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb3c0ff",
   "metadata": {},
   "source": [
    "## UPLOADING DATA TO POSTGRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2418f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting upload at: 20:57:38.623277\n",
      "Data from summary_MICRODADOS_ENEM_2020.csv appended to PostgreSQL\n",
      "Data from summary_MICRODADOS_ENEM_2021.csv appended to PostgreSQL\n"
     ]
    }
   ],
   "source": [
    "current_time = datetime.now().time()\n",
    "print(\"Starting upload at:\", current_time)\n",
    "\n",
    "# Specify the S3 bucket name and file names\n",
    "bucket_name = 'inep-cleaned'\n",
    "file_names = [\n",
    "    'summary_MICRODADOS_ENEM_2020.csv',\n",
    "    'summary_MICRODADOS_ENEM_2021.csv',\n",
    "    'summary_MICRODADOS_ENEM_2022.csv'\n",
    "]\n",
    "\n",
    "# Set up a connection to your PostgreSQL RDS instance\n",
    "host = 'database-2.ckuaogoaistw.us-east-1.rds.amazonaws.com'\n",
    "port = 5432\n",
    "database = 'postgres'\n",
    "user = 'postgres'\n",
    "password = 'postgres'\n",
    "\n",
    "# Iterate through the specified file names and upload each DataFrame separately\n",
    "for file_name in file_names:\n",
    "\n",
    "    # Construct the public S3 URL\n",
    "    s3_url = f'https://{bucket_name}.s3.amazonaws.com/{file_name}'\n",
    "    \n",
    "    # Download the CSV file using requests\n",
    "    response = requests.get(s3_url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Read the CSV data into a pandas DataFrame\n",
    "        df = pd.read_csv(StringIO(response.text))\n",
    "        \n",
    "        # Set up a SQLAlchemy engine for the current DataFrame\n",
    "        engine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{database}')\n",
    "        \n",
    "        table_name='inep_data'\n",
    "        \n",
    "        # Use Pandas to append data to the existing table\n",
    "        df.to_sql(\n",
    "            table_name,\n",
    "            engine,\n",
    "            if_exists='append',  # Append data to the existing table\n",
    "            index=False  # Set to False if you don't want to include the DataFrame index as a column\n",
    "        )\n",
    "        current_time = datetime.now().time()\n",
    "        print(f\"Data from {file_name} appended to PostgreSQL at {current_time}\")\n",
    "    else:\n",
    "        print(f\"Failed to download {file_name}\")\n",
    "\n",
    "current_time = datetime.now().time()\n",
    "print(\"Finishing upload at:\", current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53caee5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
